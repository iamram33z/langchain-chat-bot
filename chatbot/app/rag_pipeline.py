import os
from dotenv import load_dotenv
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_community.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from huggingface_hub import InferenceClient

# Load environment variables
load_dotenv()
HUGGINGFACE_API_KEY = os.getenv("HUGGINGFACE_API_KEY")

# Debugging: Check if the Hugging Face API key is loaded correctly
print("HUGGINGFACE_API_KEY:", HUGGINGFACE_API_KEY)

# Constants
PDF_PATH = os.path.join(os.getcwd(), "chatbot/data/codeprolk.pdf")
LLM_MODEL = "meta-llama/Llama-3.2-1B"
EMBEDDING_MODEL = "sentence-transformers/all-mpnet-base-v2"

# Ensure the PDF file exists
if not os.path.exists(PDF_PATH):
    raise FileNotFoundError(f"PDF file not found at {PDF_PATH}")
print("PDF_PATH:", PDF_PATH)  # Debugging line to check if the PDF path is correct

# Initialize Embeddings using HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)

def load_and_preprocess_pdf(pdf_path):
    """
    Loads the PDF, splits the text into chunks, and initializes FAISS vector store.
    
    Args:
        pdf_path (str): Path to the PDF file.
        
    Returns:
        FAISS vector store retriever.
    """
    # Load PDF
    loader = PyPDFLoader(pdf_path)
    documents = loader.load()

    # Split text into smaller chunks for better retrieval
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)
    chunks = text_splitter.split_documents(documents)

    # Create FAISS vector store for document retrieval
    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)
    return vectorstore.as_retriever()

# Load the retriever from preprocessed PDF
retriever = load_and_preprocess_pdf(PDF_PATH)

# Initialize Hugging Face LLM Client
llm_client = InferenceClient(model=LLM_MODEL, token=HUGGINGFACE_API_KEY)

# Define Prompt Template
prompt_template = """
You are a helpful assistant. Answer the question based on the provided context.

Question: {question}

Context: {context}

Answer:
"""
prompt = ChatPromptTemplate.from_template(prompt_template)

def retrieve_context(question: str):
    """
    Retrieves relevant context from the vector store based on the question.
    
    Args:
        question (str): The user's question.
        
    Returns:
        str: The relevant context to provide to the LLM.
    """
    documents = retriever.invoke(question)
    context = "\n\n".join([doc.page_content for doc in documents])
    return context

def query_llm(input_dict: dict):
    """
    Calls Hugging Face Inference API with extracted question and context.
    
    Args:
        input_dict (dict): Dictionary containing the question and context.
        
    Returns:
        str: The response from the LLM.
    """
    question = input_dict["question"]
    context = input_dict["context"]
    
    # Render the prompt into a string
    prompt_string = prompt_template.format(question=question, context=context)
    
    # Call Hugging Face Inference API
    response = llm_client.text_generation(prompt_string, max_new_tokens=200)
    return response

# Define the RAG pipeline
rag_chain = (
    {"context": RunnableLambda(retrieve_context), "question": RunnablePassthrough()}
    | RunnableLambda(query_llm)
)

def get_rag_response(question: str):
    """
    Executes the RAG pipeline with a user question.
    
    Args:
        question (str): The question asked by the user.
        
    Returns:
        str: The answer generated by the RAG pipeline.
    """
    return rag_chain.invoke(question)