{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/fastapi-chatbot\n",
    "‚îÇ‚îÄ‚îÄ /data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ codeprolk.pdf  # Your PDF\n",
    "‚îÇ‚îÄ‚îÄ main.py            # FastAPI app\n",
    "‚îÇ‚îÄ‚îÄ rag_pipeline.py    # RAG implementation\n",
    "‚îÇ‚îÄ‚îÄ requirements.txt   # Dependencies\n",
    "‚îÇ‚îÄ‚îÄ .env               # API key storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üöÄ Code Implementation\n",
    "üîπ 1Ô∏è‚É£ rag_pipeline.py (LangChain & FAISS Retrieval)\n",
    "This module handles:\n",
    "\n",
    "Loading PDFs\n",
    "\n",
    "Chunking & Vector Storage\n",
    "\n",
    "Querying with Hugging Face API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from huggingface_hub import InferenceClient\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "HUGGINGFACE_API_KEY = os.getenv(\"HUGGINGFACE_API_KEY\")\n",
    "\n",
    "# Constants\n",
    "PDF_PATH = os.path.join(os.path.dirname(os.getcwd()), \"data/codeprolk.pdf\")\n",
    "LLM_MODEL = \"meta-llama/Llama-3.2-1B\"\n",
    "EMBEDDING_MODEL = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "\n",
    "# Initialize Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)\n",
    "\n",
    "def load_and_preprocess_pdf(pdf_path):\n",
    "    \"\"\"Loads PDF, splits text, and initializes FAISS vector store.\"\"\"\n",
    "    loader = PyPDFLoader(pdf_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=400, chunk_overlap=50)\n",
    "    chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "    vectorstore = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# Load Retriever\n",
    "retriever = load_and_preprocess_pdf(PDF_PATH)\n",
    "\n",
    "# Initialize Hugging Face LLM Client\n",
    "llm_client = InferenceClient(model=LLM_MODEL, token=HUGGINGFACE_API_KEY)\n",
    "\n",
    "# Define Prompt Template\n",
    "prompt_template = \"\"\"\n",
    "You are a helpful assistant. Answer the question based on the provided context.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Context: {context}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(prompt_template)\n",
    "\n",
    "def retrieve_context(input_dict):\n",
    "    \"\"\"Retrieves relevant context from vector store.\"\"\"\n",
    "    question = input_dict[\"question\"]\n",
    "    documents = retriever.invoke(question)\n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    return context\n",
    "\n",
    "def query_llm(input_text):\n",
    "    \"\"\"Calls Hugging Face Inference API.\"\"\"\n",
    "    response = llm_client.text_generation(input_text, max_new_tokens=200)\n",
    "    return response\n",
    "\n",
    "# Define RAG Pipeline (FIXED)\n",
    "rag_chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_context), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | RunnableLambda(query_llm)  # ‚úÖ Wrap LLM call in RunnableLambda\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "#from rag_pipeline import rag_chain\n",
    "\n",
    "# Initialize FastAPI\n",
    "app = FastAPI(title=\"LangChain Chatbot API\", version=\"1.0\")\n",
    "\n",
    "# Request Model\n",
    "class QueryRequest(BaseModel):\n",
    "    question: str\n",
    "\n",
    "@app.post(\"/query\")\n",
    "async def query_rag(request: QueryRequest):\n",
    "    \"\"\"Handles RAG-based question answering.\"\"\"\n",
    "    try:\n",
    "        response = rag_chain.invoke({\"question\": request.question})\n",
    "        return {\"answer\": response}\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "\n",
    "# Root Endpoint\n",
    "@app.get(\"/\")\n",
    "async def root():\n",
    "    return {\"message\": \"Welcome to the LangChain Chatbot API\"}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
