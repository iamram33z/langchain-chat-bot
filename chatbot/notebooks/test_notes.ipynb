{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from langchain_huggingface import HuggingFaceEndpoint, HuggingFaceEmbeddings\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "\n",
    "# Import Other Libraries\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Environment Variables\n",
    "load_dotenv()\n",
    "\n",
    "# Set Environment Variables\n",
    "HUGGING_FACE_API = os.getenv(\"HUGGINGFACE_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model URLs\n",
    "meta_llama = \"meta-llama/Llama-3.2-1B\"\n",
    "SentenceTransformer = \"sentence-transformers/all-mpnet-base-v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize HuggingFace LLM\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=meta_llama,  # Llama model\n",
    "    huggingfacehub_api_token=HUGGING_FACE_API,  # Pass the API key here\n",
    "    temperature=0.5,\n",
    "    max_new_tokens=512,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'from langchain_huggingface import HuggingFaceEndpoint\\n\\n# Initialize HuggingFace LLM\\nllm = HuggingFaceEndpoint(\\n    repo_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Public model\\n    huggingfacehub_api_token=HUGGING_FACE_API,  # Pass the API key here\\n)\\n\\n# Test the LLM\\nprint(llm(\"What is the capital of France?\"))'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"from langchain_huggingface import HuggingFaceEndpoint\n",
    "\n",
    "# Initialize HuggingFace LLM\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",  # Public model\n",
    "    huggingfacehub_api_token=HUGGING_FACE_API,  # Pass the API key here\n",
    ")\n",
    "\n",
    "# Test the LLM\n",
    "print(llm(\"What is the capital of France?\"))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is valid. User: iammudaser\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import HfApi\n",
    "\n",
    "api = HfApi(token=HUGGING_FACE_API)\n",
    "try:\n",
    "    user_info = api.whoami()\n",
    "    print(\"API Key is valid. User:\", user_info[\"name\"])\n",
    "except Exception as e:\n",
    "    print(\"Invalid API Key:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize HuggingFace Embeddings\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=SentenceTransformer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initilize Output Parser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF File Path: c:\\Users\\RameezRassdeen\\Documents\\langchain-chat-bot\\chatbot\\data/codeprolk.pdf\n"
     ]
    }
   ],
   "source": [
    "# Print PDF file path\n",
    "pdf_file_path = os.path.join(os.path.dirname(os.getcwd()), \"data/codeprolk.pdf\")\n",
    "print(\"PDF File Path:\", pdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\RameezRassdeen\\\\Documents\\\\langchain-chat-bot\\\\chatbot\\\\data/codeprolk.pdf'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load PDF File\n",
    "loader = PyPDFLoader(\"C:\\\\Users\\\\RameezRassdeen\\\\Documents\\\\langchain-chat-bot\\\\chatbot\\\\data\\\\codeprolk.pdf\")\n",
    "\n",
    "# Load documents from the PDF\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of chunks created: 20\n"
     ]
    }
   ],
   "source": [
    "# Initialize Text Splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "# Split documents into smaller chunks\n",
    "texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# Print the number of chunks created\n",
    "print(f\"Number of chunks created: {len(texts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a FAISS vector store from the texts\n",
    "vectorstore = FAISS.from_documents(documents=texts, embedding=embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a prompt template for the LLM\n",
    "prompt_template =\"\"\"\n",
    "    You are a helpful assistant. Answer the question based on the provided context.\n",
    "\n",
    "    Question: {question}\n",
    "\n",
    "    Context: {context}\n",
    "\n",
    "    Answer: \n",
    "    \"\"\"\n",
    "\n",
    "# Create the prompt from the template\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to extract the question and pass it to the retriever\n",
    "def retrieve_context(input_dict):\n",
    "    question = input_dict[\"question\"]\n",
    "    context = retriever.invoke(question)\n",
    "    print(f\"Retrieved context: {context}\")  # Check the context being passed\n",
    "    return context\n",
    "\n",
    "# Update the chain\n",
    "chain = (\n",
    "    {\"context\": RunnableLambda(retrieve_context), \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | output_parser\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved context: [Document(id='263a197e-4cf4-4a1b-87d5-812442a78afb', metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2024-06-18T10:00:42+05:30', 'author': 'Dinesh Piyasamara', 'moddate': '2024-06-18T10:00:42+05:30', 'source': 'C:\\\\Users\\\\RameezRassdeen\\\\Documents\\\\langchain-chat-bot\\\\chatbot\\\\data\\\\codeprolk.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4'}, page_content='Partnerships and Collaborations \\nCodePRO LK is exploring partnerships with educational institutions, tech companies, and \\nindustry experts to enrich its content and provide learners with access to a broader range of \\nresources and opportunities. These collaborations aim to bridge the gap between education and \\nindustry, ensuring that learners are well-prepared for real-world challenges.'), Document(id='1fdd39a1-3f55-4ad3-8962-cf71134ef370', metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2024-06-18T10:00:42+05:30', 'author': 'Dinesh Piyasamara', 'moddate': '2024-06-18T10:00:42+05:30', 'source': 'C:\\\\Users\\\\RameezRassdeen\\\\Documents\\\\langchain-chat-bot\\\\chatbot\\\\data\\\\codeprolk.pdf', 'total_pages': 4, 'page': 1, 'page_label': '2'}, page_content='Community and Support \\nCodePRO LK has cultivated a vibrant community where learners can interact, share insights, and \\nsupport each other. Additionally, the platform offers consultation services for personalized \\nlearning support. \\n \\nCodePRO LK YouTube Channel \\nOverview \\nThe CodePRO LK YouTube Channel is a crucial extension of the platform, providing a wealth'), Document(id='1a1c4dca-3743-4df0-ad7d-cadb105e4718', metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2024-06-18T10:00:42+05:30', 'author': 'Dinesh Piyasamara', 'moddate': '2024-06-18T10:00:42+05:30', 'source': 'C:\\\\Users\\\\RameezRassdeen\\\\Documents\\\\langchain-chat-bot\\\\chatbot\\\\data\\\\codeprolk.pdf', 'total_pages': 4, 'page': 3, 'page_label': '4'}, page_content='Community Engagement and Events \\nCodePRO LK is committed to strengthening its community through regular engagement \\nactivities such as webinars, live coding sessions, hackathons, and tech talks. These events \\nprovide valuable networking opportunities and practical experience, fostering a supportive and \\ncollaborative learning environment. \\n \\nConclusion'), Document(id='12f0ede8-2042-4ee2-ac2f-98a8817db0f9', metadata={'producer': 'Microsoft® Word for Microsoft 365', 'creator': 'Microsoft® Word for Microsoft 365', 'creationdate': '2024-06-18T10:00:42+05:30', 'author': 'Dinesh Piyasamara', 'moddate': '2024-06-18T10:00:42+05:30', 'source': 'C:\\\\Users\\\\RameezRassdeen\\\\Documents\\\\langchain-chat-bot\\\\chatbot\\\\data\\\\codeprolk.pdf', 'total_pages': 4, 'page': 2, 'page_label': '3'}, page_content='best aiya.\" \\n• Heshan R: \"Great work brother. I was scared of coding before I attended this course. \\nHowever, you taught us A-Z in Python. Thanks again for volunteering for such a thing. \\nGood luck.❤\" \\nThese testimonials highlight the significant positive impact CodePRO LK has had on its learners, \\nhelping them overcome challenges and achieve their educational and professional goals.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\RameezRassdeen\\Documents\\langchain-chat-bot\\.venv\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:131: FutureWarning: 'post' (from 'huggingface_hub.inference._client') is deprecated and will be removed from version '0.31.0'. Making direct POST requests to the inference server is not supported anymore. Please use task methods instead (e.g. `InferenceClient.chat_completion`). If your use case is not supported, please open an issue in https://github.com/huggingface/huggingface_hub.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: \n"
     ]
    }
   ],
   "source": [
    "# Invoke RAG Chain with a sample question\n",
    "response = chain.invoke({\"question\": \"who is codeprolk?\"})\n",
    "\n",
    "# Print the response\n",
    "print(\"Response:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
